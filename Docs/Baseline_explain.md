# ì „ì²˜ë¦¬(Preprocessing)

ML ëª¨ë¸ì€ ë°ì´í„°ë¡œë¶€í„° ì–´ë– í•œ íŒ¨í„´ì´ë‚˜ íŠ¹ì§•ì„ ì°¾ìŠµë‹ˆë‹¤. 
ê·¸ëŸ°ë° ë°ì´í„°ê°€ ë„ˆë¬´ ë“¤ì­‰ë‚ ì­‰ì´ë©´ íŒ¨í„´ì„ ì°¾ê¸° í˜ë“¤ì§€ ì•Šì„ê¹Œìš”?

ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ì „ì²˜ë¦¬ë¥¼ í•´ì•¼í•©ë‹ˆë‹¤. ê°‘ìê¸° ë„ˆë¬´ í° ìˆ˜ê°€ ë“¤ì–´ ìˆê±°ë‚˜ ê°’ì´ ì—†ëŠ” ê²½ìš°ê°€ ìˆë‹¤ë©´
ì´ë¥¼ ì œê±°í•˜ê±°ë‚˜ ë³´ì •í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. 

ë‹¤ìŒ í‘œë¥¼ í•œë²ˆ ë³¼ê¹Œìš”?
<p align="center">
  <img src="../asset/before_preprocess.jpg" alt="before" width="100%"/>
</p>
ì•— Mileageë¥¼ ì´ìš©í•˜ì—¬ ì´ ì£¼í–‰ ê±°ë¦¬ë¥¼ ì´ìš©í•´ ì¤‘ê³ ì°¨ì˜ ê°€ê²©ì„ ì¶”ë¡ í•˜ë ¤í–ˆëŠ”ë°
ìˆ«ìê°€ ì•„ë‹™ë‹ˆë‹¤! 143131 kmì™€ ê°™ì´ kmì´ ë¶™ì€ ë¬¸ìì—´ì´ë„¤ìš”.
ê·¸ëŸ¼ ì´ê±¸ ìˆ«ìë¡œ ë°”ê¿”ì•¼ í•©ë‹ˆë‹¤. 

ë‹¤ìŒì˜ ì½”ë“œë¥¼ í•œë²ˆ ë³¼ê¼ê»˜ìš”. Baseline/data_preprocessing.py íŒŒì¼ì— ìˆëŠ” ì½”ë“œ ì¼ë¶€ë¶„ì…ë‹ˆë‹¤. 
```python
df["Mileage"] = df["Mileage"].str.replace(" km", "", regex=True).astype(int)
```
ì´ ì½”ë“œëŠ” dfë¼ëŠ” ë°ì´í„° ì§‘í•©ì˜ Mileageë¼ëŠ” ì—´ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ í›„, 
" km"ì„ ""(ì•„ë¬´ê²ƒë„ ì—†ìŒ)ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

# í•™ìŠµ

í•™ìŠµì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ ì°¾ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì¤€ë¹„í•œ ê¹¨ë—í•œ ë°ì´í„°ë¥¼ ëª¨ë¸ì—ê²Œ ë³´ì—¬ì£¼ê³ ,
ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ê·œì¹™ì„ ì°¾ë„ë¡ í•˜ëŠ” ê²ƒì´ì£ .

ì˜ˆë¥¼ ë“¤ì–´, ì¤‘ê³ ì°¨ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ì€:
- ì—°ì‹ì´ ì˜¤ë˜ë ìˆ˜ë¡ ê°€ê²©ì´ ë‚®ì•„ì§„ë‹¤
- ì£¼í–‰ê±°ë¦¬ê°€ ë§ì„ìˆ˜ë¡ ê°€ê²©ì´ ë‚®ì•„ì§„ë‹¤
- ê³ ê¸‰ ë¸Œëœë“œì¼ìˆ˜ë¡ ê°€ê²©ì´ ë†’ë‹¤

ì´ëŸ° íŒ¨í„´ë“¤ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

## í•™ìŠµ ì½”ë“œ ì˜ˆì‹œ
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# ë°ì´í„° ë¶„í•  (í•™ìŠµìš© 80%, ê²€ì¦ìš© 20%)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = LinearRegression()
model.fit(X_train, y_train)

# ì˜ˆì¸¡
predictions = model.predict(X_val)
```

# ê²°ê³¼ ë¶„ì„

ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ í•™ìŠµí–ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. 
ë‹¨ìˆœíˆ ì •í™•ë„ë§Œ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì–´ë–¤ ë¶€ë¶„ì—ì„œ ì‹¤ìˆ˜ë¥¼ í•˜ëŠ”ì§€, 
ì–´ë–¤ íŠ¹ì§•ì´ ê°€ì¥ ì¤‘ìš”í•œì§€ ë“±ì„ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤.

## í‰ê°€ ì§€í‘œ
- **MAE (Mean Absolute Error)**: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì ˆëŒ€ ì°¨ì´ì˜ í‰ê· 
- **RMSE (Root Mean Squared Error)**: ì˜¤ì°¨ ì œê³±ì˜ í‰ê· ì˜ ì œê³±ê·¼
- **RÂ² Score**: ëª¨ë¸ì´ ë°ì´í„°ì˜ ë¶„ì‚°ì„ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€

## ì‹œê°í™”ë¥¼ í†µí•œ ë¶„ì„
```python
import matplotlib.pyplot as plt

# ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’ í”Œë¡¯
plt.figure(figsize=(10, 6))
plt.scatter(y_val, predictions, alpha=0.5)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Prediction vs Actual')
plt.show()

# ì”ì°¨(residual) ë¶„ì„
residuals = y_val - predictions
plt.figure(figsize=(10, 6))
plt.scatter(predictions, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Price')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()
```

## Feature Importance ë¶„ì„
ì–´ë–¤ íŠ¹ì§•ì´ ê°€ê²© ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•œì§€ í™•ì¸:
```python
# Random Forestì˜ ê²½ìš°
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("Top 10 Important Features:")
print(feature_importance.head(10))
```

# ê°œì„  ë°©í–¥

1. **ë” ë‚˜ì€ ì „ì²˜ë¦¬**
   - ì´ìƒì¹˜ ì œê±°
   - ìŠ¤ì¼€ì¼ë§ (MinMaxScaler, StandardScaler)
   - ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ê°œì„ 

2. **Feature Engineering**
   - ìƒˆë¡œìš´ íŠ¹ì§• ìƒì„± (ì°¨ëŸ‰ ë‚˜ì´, ë¸Œëœë“œë³„ í‰ê·  ê°€ê²© ë“±)
   - íŠ¹ì§• ì„ íƒ (ë¶ˆí•„ìš”í•œ íŠ¹ì§• ì œê±°)

3. **ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©**
   - Random Forest
   - XGBoost
   - LightGBM

4. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
   - GridSearchCV
   - RandomizedSearchCV
   - Optuna

5. **ì•™ìƒë¸” ê¸°ë²•**
   - Voting
   - Stacking
   - Blending

ì´ì œ ì—¬ëŸ¬ë¶„ë§Œì˜ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ì„ ê°œì„ í•´ë³´ì„¸ìš”! ğŸš€